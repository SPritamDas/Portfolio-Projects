{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pinecone pypdf2 --quiet"
      ],
      "metadata": {
        "id": "RTKOQKMcJ66g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "36d3235c-a5ca-4b3e-fa68-33fb11b35733"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/427.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/87.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.7/87.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n"
      ],
      "metadata": {
        "id": "AUNJ12-1dy-w"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tqdm\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import google.generativeai as genai\n",
        "import time\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from PyPDF2 import PdfReader\n",
        "import re\n",
        "_RE_COMBINE_WHITESPACE = re.compile(r\"(?a:\\s+)\")\n",
        "_RE_STRIP_WHITESPACE = re.compile(r\"(?a:^\\s+|\\s+$)\")\n",
        "genai.configure(api_key=userdata.get('GEMINI_API_KEY'))\n",
        "pc = Pinecone(api_key=userdata.get('PINECONE_API_KEY'))"
      ],
      "metadata": {
        "id": "ycd7j1x0KRN2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "index_name = 'mlchatbotqa'\n",
        "# pc.create_index(\n",
        "#     name=index_name,\n",
        "#     dimension=768 ,\n",
        "#     metric=\"cosine\",\n",
        "#     spec=ServerlessSpec(\n",
        "#         cloud=\"aws\",\n",
        "#         region=\"us-east-1\"\n",
        "#     )\n",
        "# )"
      ],
      "metadata": {
        "id": "KnJ2Wz39KzpW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while not pc.describe_index(index_name).status['ready']:\n",
        "    time.sleep(1)\n",
        "\n",
        "index = pc.Index(index_name)"
      ],
      "metadata": {
        "id": "OgHUQ5O0LIkC"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader = PdfReader('/content/Introduction to Machine Learning with Python ( PDFDrive.com )-min.pdf')\n",
        "number_of_pages = len(reader.pages)\n",
        "\n",
        "# for i in tqdm.tqdm(range(0,number_of_pages)):\n",
        "#     text_in_page = ''\n",
        "#     page = reader.pages[i]\n",
        "#     text_in_page = page.extract_text()\n",
        "#     text_in_page = text_in_page.replace('\\n',' ')\n",
        "#     text_in_page = _RE_COMBINE_WHITESPACE.sub(\" \", text_in_page)\n",
        "#     text_in_page = _RE_STRIP_WHITESPACE.sub(\"\", text_in_page)\n",
        "#     if(len(text_in_page)>9000):\n",
        "#         text_in_page = text_in_page[:9000]\n",
        "#     gemini_embedding_result= genai.embed_content(\n",
        "#         model=\"models/text-embedding-004\",\n",
        "#         content= text_in_page,\n",
        "#         task_type=\"retrieval_document\",\n",
        "#         title=\"Embedding of single string\")\n",
        "#     index.upsert(\n",
        "#         vectors=[\n",
        "#             {\n",
        "#             \"id\": 'page-' + str(i) + '-'+ str(i+1),\n",
        "#             \"values\": gemini_embedding_result['embedding'] ,\n",
        "#             \"metadata\": {\"text\":text_in_page}\n",
        "#             }\n",
        "#         ]\n",
        "#     )\n",
        "\n",
        "from math import ceil\n",
        "\n",
        "for i in tqdm.tqdm(range(14, number_of_pages)):\n",
        "    text_in_page = ''\n",
        "    page = reader.pages[i]\n",
        "    text_in_page = page.extract_text()\n",
        "    text_in_page = text_in_page.replace('\\n', ' ')\n",
        "    text_in_page = _RE_COMBINE_WHITESPACE.sub(\" \", text_in_page)\n",
        "    text_in_page = _RE_STRIP_WHITESPACE.sub(\"\", text_in_page)\n",
        "\n",
        "    # Splitting the text of any page into 5 parts for better result\n",
        "    num_parts = 1\n",
        "    chunk_size = ceil(len(text_in_page) / num_parts)\n",
        "\n",
        "    for j in range(num_parts):\n",
        "        start_idx = j * chunk_size\n",
        "        end_idx = min((j + 1) * chunk_size, len(text_in_page))\n",
        "        chunk = text_in_page[start_idx:end_idx]\n",
        "\n",
        "\n",
        "        if len(chunk) > 0:\n",
        "            gemini_embedding_result = genai.embed_content(\n",
        "                model=\"models/text-embedding-004\",\n",
        "                content=chunk,\n",
        "                task_type=\"retrieval_document\",\n",
        "                title=f\"Embedding of page {i+1}, part {j+1}\"\n",
        "            )\n",
        "        index.upsert(\n",
        "                vectors=[\n",
        "                    {\n",
        "                        \"id\": f'page-{i+1}-part-{j+1}',\n",
        "                        \"values\": gemini_embedding_result['embedding'],\n",
        "                        \"metadata\": {\"text\": chunk}\n",
        "                    }\n",
        "                ]\n",
        "            )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "r8eDfmEELQZo",
        "outputId": "2cdeec9c-460e-439f-8aac-81f04ac715e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 378/378 [12:36<00:00,  2.00s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = \"who is the president of America\""
      ],
      "metadata": {
        "id": "7zHtWHqZNTKN"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectordb_top_matches = ''\n",
        "gemini_embedding_question= genai.embed_content(\n",
        "        model=\"models/text-embedding-004\",\n",
        "        content=  question,\n",
        "        task_type=\"retrieval_document\",\n",
        "        title=\"Embedding of single string\")\n",
        "\n",
        "\n",
        "query_result = index.query(\n",
        "            vector = gemini_embedding_question['embedding'],\n",
        "            top_k = 5,\n",
        "            include_metadata=True)\n",
        "for result_pinecone in query_result['matches']:\n",
        "    vectordb_top_matches += result_pinecone['metadata']['text']\n",
        "\n",
        "generation_config = {\n",
        "  \"temperature\": 0.4,\n",
        "  \"top_p\": 0.95,\n",
        "  \"top_k\": 64,\n",
        "  \"max_output_tokens\": 8192,\n",
        "  \"response_mime_type\": \"text/plain\",\n",
        "}\n",
        "\n",
        "gen_model = genai.GenerativeModel(\n",
        "  model_name=\"gemini-1.5-flash\",\n",
        "  generation_config=generation_config,\n",
        ")"
      ],
      "metadata": {
        "id": "qlXhZ4xoMtaC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(query_result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9tZ4SykNp-d",
        "outputId": "c0fc7211-b696-4ff6-b570-9e25478ed13a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'matches': [{'id': 'page-163-part-1',\n",
            "              'metadata': {'text': 'Out[23]: Alejandro Toledo 39 Alvaro Uribe '\n",
            "                                   '35 Amelie Mauresmo 21 Andre Agassi '\n",
            "                                   '36Angelina Jolie 20 Arnold Schwarzenegger '\n",
            "                                   '42Atal Bihari Vajpayee 24 Bill Clinton '\n",
            "                                   '29Carlos Menem 21 Colin Powell 236David '\n",
            "                                   'Beckham 31 Donald Rumsfeld 121George W '\n",
            "                                   'Bush 530 George Robertson 22Gerhard '\n",
            "                                   'Schroeder 109 Gloria Macapagal Arroyo '\n",
            "                                   '44Gray Davis 26 Guillermo Coria 30Hamid '\n",
            "                                   'Karzai 22 Hans Blix 39Hugo Chavez 71 Igor '\n",
            "                                   'Ivanov 20[...] [...]Laura Bush 41 Lindsay '\n",
            "                                   'Davenport 22Lleyton Hewitt 41 Luiz Inacio '\n",
            "                                   'Lula da Silva 48Mahmoud Abbas 29 Megawati '\n",
            "                                   'Sukarnoputri 33Michael Bloomberg 20 Naomi '\n",
            "                                   'Watts 22Nestor Kirchner 37 Paul Bremer '\n",
            "                                   '20Pete Sampras 22 Recep Tayyip Erdogan '\n",
            "                                   '30Ricardo Lagos 27 Roh Moo-hyun 32Rudolph '\n",
            "                                   'Giuliani 26 Saddam Hussein 23Serena '\n",
            "                                   'Williams 52 Silvio Berlusconi 33Tiger '\n",
            "                                   'Woods 23 Tom Daschle 25Tom Ridge 33 Tony '\n",
            "                                   'Blair 144Vicente Fox 32 Vladimir Putin '\n",
            "                                   '49Winona Ryder 24 To make the data less '\n",
            "                                   'skewed, we will only take up to 50 images '\n",
            "                                   'of each person (otherwise, the feature '\n",
            "                                   'extraction would be overwhelmed by the '\n",
            "                                   'likelihood of George W . Bush): In[24]: '\n",
            "                                   'mask = np.zeros(people.target.shape, '\n",
            "                                   'dtype=np.bool) for target in '\n",
            "                                   'np.unique(people.target): '\n",
            "                                   'mask[np.where(people.target == '\n",
            "                                   'target)[0][:50]] = 1 X_people = '\n",
            "                                   'people.data[mask] y_people = '\n",
            "                                   'people.target[mask] # scale the grayscale '\n",
            "                                   'values to be between 0 and 1 # instead of '\n",
            "                                   '0 and 255 for better numeric '\n",
            "                                   'stabilityX_people = X_people / 255. A '\n",
            "                                   'common task in face recognition is to ask '\n",
            "                                   'if a previously unseen face belongs to a '\n",
            "                                   'known person from a database. This has '\n",
            "                                   'applications in photo collection, social '\n",
            "                                   'media, and security applications. One way '\n",
            "                                   'to solve this problem would be to build a '\n",
            "                                   'classifier where each person is a separate '\n",
            "                                   'class. However, there are usually many '\n",
            "                                   'dif‐ ferent people in face databases, and '\n",
            "                                   'very few images of the same person (i.e., '\n",
            "                                   'very few training examples per class). '\n",
            "                                   'That makes it hard to train most '\n",
            "                                   'classifiers. Additionally, Dimensionality '\n",
            "                                   'Reduction, Feature Extraction, and '\n",
            "                                   'Manifold Learning | 149'},\n",
            "              'score': 0.612401783,\n",
            "              'values': []},\n",
            "             {'id': 'page-339-part-1',\n",
            "              'metadata': {'text': '1Arguably, the content of websites linked '\n",
            "                                   'to in tweets contains more information '\n",
            "                                   'than the text of the tweets themselves. '\n",
            "                                   '2Most of what we will talk about in the '\n",
            "                                   'rest of the chapter also applies to other '\n",
            "                                   'languages that use the Romanalphabet, and '\n",
            "                                   'partially to other languages with word '\n",
            "                                   'boundary delimiters. Chinese, for example, '\n",
            "                                   'does notdelimit word boundaries, and has '\n",
            "                                   'other challenges that make applying the '\n",
            "                                   'techniques in this chapter difficult. 3The '\n",
            "                                   'dataset is available at '\n",
            "                                   'http://ai.stanford.edu/~amaas/data/sentiment/ '\n",
            "                                   '.numbers, or other identifiers. These '\n",
            "                                   'kinds of strings are often very hard to '\n",
            "                                   'parse, and their treatment is highly '\n",
            "                                   'dependent on context and domain. A '\n",
            "                                   'systematic treatment of these cases is '\n",
            "                                   'beyond the scope of this book. The final '\n",
            "                                   'category of string data is freeform text '\n",
            "                                   'data that consists of phrases or sen‐ '\n",
            "                                   'tences. Examples include tweets, chat '\n",
            "                                   'logs, and hotel reviews, as well as the '\n",
            "                                   'collected works of Shakespeare, the '\n",
            "                                   'content of Wikipedia, or the Project '\n",
            "                                   'Gutenberg collection of 50,000 ebooks. All '\n",
            "                                   'of these collections contain information '\n",
            "                                   'mostly as sentences composed of words.1 '\n",
            "                                   'For simplicity’s sake, let’s assume all '\n",
            "                                   'our documents are in one language, '\n",
            "                                   'English.2 In the context of text analysis, '\n",
            "                                   'the dataset is often called the cor‐ pus, '\n",
            "                                   'and each data point, represented as a '\n",
            "                                   'single text, is called a document . These '\n",
            "                                   'terms come from the information retrieval '\n",
            "                                   '(IR) and natural language processing (NLP) '\n",
            "                                   'community, which both deal mostly in text '\n",
            "                                   'data. Example Application: Sentiment '\n",
            "                                   'Analysis of Movie Reviews As a running '\n",
            "                                   'example in this chapter, we will use a '\n",
            "                                   'dataset of movie reviews from the IMDb '\n",
            "                                   '(Internet Movie Database) website '\n",
            "                                   'collected by Stanford researcher Andrew '\n",
            "                                   'Maas.3 This dataset contains the text of '\n",
            "                                   'the reviews, together with a label that '\n",
            "                                   'indi‐ cates whether a review is “positive” '\n",
            "                                   'or “negative. ” The IMDb website itself '\n",
            "                                   'contains ratings from 1 to 10. To simplify '\n",
            "                                   'the modeling, this annotation is '\n",
            "                                   'summarized as a two-class classification '\n",
            "                                   'dataset where reviews with a score of 6 or '\n",
            "                                   'higher are labeled as positive, and the '\n",
            "                                   'rest as negative. We will leave the '\n",
            "                                   'question of whether this is a good '\n",
            "                                   'representation of the data open, and '\n",
            "                                   'simply use the data as provided by Andrew '\n",
            "                                   'Maas. After unpacking the data, the '\n",
            "                                   'dataset is provided as text files in two '\n",
            "                                   'separate folders, one for the training '\n",
            "                                   'data and one for the test data. Each of '\n",
            "                                   'these in turn has two sub‐ folders, one '\n",
            "                                   'called pos and one called neg: Example '\n",
            "                                   'Application: Sentiment Analysis of Movie '\n",
            "                                   'Reviews | 325'},\n",
            "              'score': 0.582750738,\n",
            "              'values': []},\n",
            "             {'id': 'page-337-part-1',\n",
            "              'metadata': {'text': 'CHAPTER 7 Working with Text Data In '\n",
            "                                   'Chapter 4 , we talked about two kinds of '\n",
            "                                   'features that can represent properties of '\n",
            "                                   'the data: continuous features that '\n",
            "                                   'describe a quantity, and categorical '\n",
            "                                   'features that are items from a fixed list. '\n",
            "                                   'There is a third kind of feature that can '\n",
            "                                   'be found in many applications, which is '\n",
            "                                   'text. For example, if we want to classify '\n",
            "                                   'an email message as either a legitimate '\n",
            "                                   'email or spam, the content of the email '\n",
            "                                   'will certainly contain important '\n",
            "                                   'information for this classification task. '\n",
            "                                   'Or maybe we want to learn about the '\n",
            "                                   'opinion of a politician on the topic of '\n",
            "                                   'immigration. Here, that individual’s '\n",
            "                                   'speeches or tweets might provide useful '\n",
            "                                   'information. In customer service, we often '\n",
            "                                   'want to find out if a message is a '\n",
            "                                   'complaint or an inquiry. We can use the '\n",
            "                                   'subject line and content of a message to '\n",
            "                                   'automatically determine the customer’s '\n",
            "                                   'intent, which allows us to send the '\n",
            "                                   'message to the appropriate department, or '\n",
            "                                   'even send a fully automatic reply. Text '\n",
            "                                   'data is usually represented as strings, '\n",
            "                                   'made up of characters. In any of the exam‐ '\n",
            "                                   'ples just given, the length of the text '\n",
            "                                   'data will vary. This feature is clearly '\n",
            "                                   'very differ‐ ent from the numeric features '\n",
            "                                   'that we’ve discussed so far, and we will '\n",
            "                                   'need to process the data before we can '\n",
            "                                   'apply our machine learning algorithms to '\n",
            "                                   'it. Types of Data Represented as Strings '\n",
            "                                   'Before we dive into the processing steps '\n",
            "                                   'that go into representing text data for '\n",
            "                                   'machine learning, we want to briefly '\n",
            "                                   'discuss different kinds of text data that '\n",
            "                                   'you might encounter. Text is usually just '\n",
            "                                   'a string in your dataset, but not all '\n",
            "                                   'string features should be treated as text. '\n",
            "                                   'A string feature can sometimes represent '\n",
            "                                   'categorical vari‐ ables, as we discussed '\n",
            "                                   'in Chapter 5 . There is no way to know how '\n",
            "                                   'to treat a string feature before looking '\n",
            "                                   'at the data. 323'},\n",
            "              'score': 0.581233561,\n",
            "              'values': []},\n",
            "             {'id': 'page-162-part-1',\n",
            "              'metadata': {'text': 'Figure 3-7. Some images from the Labeled '\n",
            "                                   'Faces in the Wild dataset There are 3,023 '\n",
            "                                   'images, each 87×65 pixels large, belonging '\n",
            "                                   'to 62 different people: In[22]: '\n",
            "                                   'print(\"people.images.shape: {}\" '\n",
            "                                   '.format(people.images.shape)) '\n",
            "                                   'print(\"Number of classes: {}\" '\n",
            "                                   '.format(len(people.target_names ))) '\n",
            "                                   'Out[22]: people.images.shape: (3023, 87, '\n",
            "                                   '65) Number of classes: 62 The dataset is a '\n",
            "                                   'bit skewed, however, containing a lot of '\n",
            "                                   'images of George W . Bush and Colin '\n",
            "                                   'Powell, as you can see here: In[23]: # '\n",
            "                                   'count how often each target appearscounts '\n",
            "                                   '= np.bincount (people.target) # print '\n",
            "                                   'counts next to target namesfor i, (count, '\n",
            "                                   'name) in enumerate (zip(counts, '\n",
            "                                   'people.target_names )): print(\"{0:25} '\n",
            "                                   '{1:3}\" .format(name, count), end=\\' \\') if '\n",
            "                                   '(i + 1) % 3 == 0: print() 148 | Chapter 3: '\n",
            "                                   'Unsupervised Learning and Preprocessing'},\n",
            "              'score': 0.576960146,\n",
            "              'values': []},\n",
            "             {'id': 'page-342-part-1',\n",
            "              'metadata': {'text': 'the corpus. Discarding the structure and '\n",
            "                                   'counting only word occurrences leads to '\n",
            "                                   'the mental image of representing text as a '\n",
            "                                   '“bag. ” Computing the bag-of-words '\n",
            "                                   'representation for a corpus of documents '\n",
            "                                   'consists of the following three steps: '\n",
            "                                   '1.Tokenization . Split each document into '\n",
            "                                   'the words that appear in it (called tokens '\n",
            "                                   '), for example by splitting them on '\n",
            "                                   'whitespace and punctuation. 2.Vocabulary '\n",
            "                                   'building . Collect a vocabulary of all '\n",
            "                                   'words that appear in any of the documents, '\n",
            "                                   'and number them (say, in alphabetical '\n",
            "                                   'order). 3.Encoding . For each document, '\n",
            "                                   'count how often each of the words in the '\n",
            "                                   'vocabu‐ lary appear in this document. '\n",
            "                                   'There are some subtleties involved in step '\n",
            "                                   '1 and step 2, which we will discuss in '\n",
            "                                   'more detail later in this chapter. For '\n",
            "                                   'now, let’s look at how we can apply the '\n",
            "                                   'bag-of-words processing using scikit-learn '\n",
            "                                   '. Figure 7-1 illustrates the process on '\n",
            "                                   'the string \"This is how you get ants.\" . '\n",
            "                                   'The output is one vector of word counts '\n",
            "                                   'for each docu‐ ment. For each word in the '\n",
            "                                   'vocabulary, we have a count of how often '\n",
            "                                   'it appears in each document. That means '\n",
            "                                   'our numeric representation has one feature '\n",
            "                                   'for each unique word in the whole dataset. '\n",
            "                                   'Note how the order of the words in the '\n",
            "                                   'original string is completely irrelevant '\n",
            "                                   'to the bag-of-words feature '\n",
            "                                   'representation. Figure 7-1. Bag-of-words '\n",
            "                                   'processing 328 | Chapter 7: Working with '\n",
            "                                   'Text Data'},\n",
            "              'score': 0.572831094,\n",
            "              'values': []}],\n",
            " 'namespace': '',\n",
            " 'usage': {'read_units': 6}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_question_prompt= f\"\"\"please act as a expert interpreater and answer the following questions {question} from the input text provided.\n",
        "If the answer is not in the input text please respond back with “Sorry, I didn’t understand your question. Do you want to connect with a live agent?” \"\"\"\n",
        "\n",
        "\n",
        "model_response = gen_model.generate_content([input_question_prompt,vectordb_top_matches])\n",
        "\n",
        "result = model_response.text\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "KZmMag0nUEtc",
        "outputId": "02cb039a-b55e-41a0-900f-3d7cb6fb3fa3"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sorry, I didn’t understand your question. Do you want to connect with a live agent?\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def question_answer():\n",
        "  question = input(\"Please enter your question: \")\n",
        "  vectordb_top_matches = ''\n",
        "  gemini_embedding_question= genai.embed_content(\n",
        "          model=\"models/text-embedding-004\",\n",
        "          content=  question,\n",
        "          task_type=\"retrieval_document\",\n",
        "          title=\"Embedding of single string\")\n",
        "\n",
        "  query_result = index.query(\n",
        "              vector = gemini_embedding_question['embedding'],\n",
        "              top_k = 5,\n",
        "              include_metadata=True)\n",
        "  for result_pinecone in query_result['matches']:\n",
        "      vectordb_top_matches += result_pinecone['metadata']['text']\n",
        "\n",
        "  generation_config = {\n",
        "    \"temperature\": 0.4,\n",
        "    \"top_p\": 0.95,\n",
        "    \"top_k\": 64,\n",
        "    \"max_output_tokens\": 8192,\n",
        "    \"response_mime_type\": \"text/plain\",\n",
        "  }\n",
        "\n",
        "  gen_model = genai.GenerativeModel(\n",
        "    model_name=\"gemini-1.5-flash\",\n",
        "    generation_config=generation_config,\n",
        "  )\n",
        "\n",
        "  input_question_prompt= f\"\"\"please act as a expert interpreater and answer the following questions {question} from the input text provided.\n",
        "  If the answer is not in the input text please respond back with “Sorry, I didn’t understand your question. Do you want to connect with a live agent?” \"\"\"\n",
        "\n",
        "\n",
        "  model_response = gen_model.generate_content([input_question_prompt,vectordb_top_matches])\n",
        "\n",
        "  result = model_response.text\n",
        "  return result\n"
      ],
      "metadata": {
        "id": "lrJ5A5YAWbRU"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Checking with Different Questions"
      ],
      "metadata": {
        "id": "5xTvfhaYYj26"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "refaCxHUXF-j",
        "outputId": "2427bd44-e93e-4e4d-8096-2e165ae114de"
      },
      "execution_count": 12,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your question: What is the classification\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The provided text focuses on **text classification**.  Specifically, it uses the example of classifying movie reviews as positive or negative, and mentions other examples like classifying emails as spam or not spam, and customer service messages as complaints or inquiries.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question_answer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "9-LyiuWtbOiz",
        "outputId": "ee428bf7-8215-4bb2-d02c-cf27b09ed432"
      },
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Please enter your question: What are the Assumptions in Linear Regression\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the provided text, a key assumption of linear regression is that the target variable (y) is a linear combination of the features (x).  The text explicitly states this as a \"strong (and somewhat unrealistic) assumption\".  The text also implies that this assumption might be less problematic with datasets containing many features.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Pc293ruUauL",
        "outputId": "6769b7d0-ac0c-4116-8e53-95c5674c6098"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.1/57.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.1/320.1 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.2/73.2 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.8/63.8 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m130.2/130.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "interface = gr.Interface(\n",
        "    fn=question_answer,\n",
        "    inputs=[\n",
        "        gr.Textbox(lines=2, placeholder=\"Enter your question here...\", label=\"Question\"),\n",
        "    ],\n",
        "    outputs=gr.Textbox(label=\"Answer\"),\n",
        "    title=\"Question-Answer Chatbot on Machine Learning Concepts\",\n",
        "    description=\"Ask a question. The chatbot will extract the answer from the given context.\"\n",
        ")\n",
        "\n",
        "interface.launch()"
      ],
      "metadata": {
        "id": "lieIGEVPbElc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "outputId": "4a63808e-ef99-4175-f82e-6022f207da5e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://f19763063c33cf3df3.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://f19763063c33cf3df3.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ]
}